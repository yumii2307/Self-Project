{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrics(artist, title):\n",
    "    base_url = 'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query='\n",
    "    url = f'{base_url}{quote(artist+title)}+%EA%B0%80%EC%82%AC'\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    lyr = soup.select_one('.lyrics_txt._lyrics_txt').get_text().strip()\n",
    "\n",
    "    return lyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "new_cvect = joblib.load('model/imdb_cvect_2.pkl')\n",
    "new_lrc = joblib.load('model/imdb_lrc2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'긍정'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = lyrics('에스파', 'life too short')\n",
    "text = re.sub('[^A-Za-z]', ' ', text)\n",
    "review_cv = new_cvect.transform([text])\n",
    "'긍정' if new_lrc.predict(review_cv)[0] == 1 else '부정'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 1 6 1 1 4 9 2 1 6 1 1 1 1 3 1 1 6 3 6 4 2 3 1 1 3 1 2 1 2 2 1 3 1 1\n",
      " 2 2 3 3 2 3 1 7 1 1 2 2 1 6 1 1 6 1 1 1 2 1 1 1 2 1 3 1 1 1 3 1 1 1]\n",
      "{'discussiontalking': 6, 'nothingyou': 33, 'say': 44, 'faceyou': 10, 'think': 56, 'words': 70, 'gospelbut': 15, 'troublei': 58, 'ain': 0, 'got': 16, 'time': 57, 'wasteyou': 64, 'need': 31, 'lifecause': 23, 'life': 22, 'shortyou': 48, 'bored': 1, 'mindyou': 29, 'really': 38, 'nonsensesomewhere': 32, 'elsecause': 9, 've': 61, 'realizei': 37, 'doing': 7, 'regardlessand': 40, 'don': 8, 'carewhat': 4, 'itand': 21, 'matterif': 27, 'like': 24, 'noti': 34, 'having': 19, 'fun': 11, 'sowhy': 50, 'stopdoing': 53, 'regardlessno': 42, 'imma': 20, 'waythat': 67, 'wanti': 63, 'stopsome': 55, 'people': 35, 'meanall': 28, 'phone': 36, 'screenwhen': 45, 'tryna': 59, 'live': 25, 'liveswhy': 26, 'gotta': 17, 'viciousbe': 62, 'business': 2, 'stead': 52, 'getting': 12, 'mineyou': 30, 'regardlessdoing': 41, 'regardlesswhy': 43, 'stopoh': 54, 'won': 69, 'turn': 60, 'glitter': 13, 'goldso': 14, 'whyare': 68, 'wasting': 65, 'somebetter': 49, 'seeds': 46, 'sowthey': 51, 'grow': 18, 'day': 5, 'buteither': 3, 'wayi': 66, 'regardless': 39, 'short': 47}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvect = CountVectorizer(stop_words='english')\n",
    "a = cvect.fit_transform([text]).toarray()[0]\n",
    "print(a)\n",
    "print(cvect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "    i == cvect.vocabulary_.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 필요한 라이브러리 가져오기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m \u001b[39mimport\u001b[39;00m summarize\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "from gensim.summarization import summarize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK 패키지의 불용어 다운로드\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 불용어 설정\n",
    "stop_words = set(stopwords.words('english')) # 사용 언어에 맞게 변경해야 함\n",
    "\n",
    "# 가사 요약 함수 정의\n",
    "def summarize_lyrics(lyrics):\n",
    "    # 텍스트 전처리\n",
    "    tokens = word_tokenize(lyrics.lower()) # 토큰화\n",
    "    tokens = [token for token in tokens if token.isalpha()] # 알파벳 문자만 포함\n",
    "    tokens = [token for token in tokens if token not in stop_words] # 불용어 제거\n",
    "\n",
    "    # 요약\n",
    "    summary = summarize(' '.join(tokens))\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# 가사 입력 받기\n",
    "input_lyrics = '''A hopeless romantic all my life\n",
    "Surrounded by couples all the time\n",
    "I guess I should take it as a sign\n",
    "(oh why oh why, oh why oh why)\n",
    "\n",
    "I’m feeling lonely (lonely)\n",
    "Oh I wish I’d find a lover\n",
    "that could hold me (hold me)\n",
    "Now I’m crying in my room\n",
    "So skeptical of love\n",
    "(say what you say but I want it more)\n",
    "But still I want it more, more, more\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb\n",
    "\n",
    "I look for his arrows everyday\n",
    "I guess he got lost or flew away\n",
    "Waiting around is a waste (waste)\n",
    "Been counting the days since November\n",
    "Is loving as good as they say?\n",
    "\n",
    "Now I’m so lonely (lonely)\n",
    "Oh I wish I’d find a lover\n",
    "that could hold me (hold me)\n",
    "Now I’m crying in my room\n",
    "So skeptical of love\n",
    "(say what you say but I want it more)\n",
    "But still I want it more, more, more\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb\n",
    "\n",
    "Hopeless girl is seeking\n",
    "someone who will share this feeling\n",
    "I’m a fool, a fool for love, a fool for love\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb'''\n",
    "\n",
    "# 가사 요약 출력\n",
    "summary = summarize_lyrics(input_lyrics)\n",
    "print(\"가사 요약:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가사 요약:\n",
      "a hopeless romantic all my life\n",
      "surrounded by couples all the time\n",
      "i guess i should take it as a sign\n",
      "(oh why oh why, oh why oh why)\n",
      "\n",
      "i’m feeling lonely (lonely)\n",
      "oh i wish i’d find a lover\n",
      "that could hold me (hold me)\n",
      "now i’m crying in my room\n",
      "so skeptical of love\n",
      "(say what you say but i want it more)\n",
      "but still i want it more, more, more\n",
      "\n",
      "i gave a second chance to cupid\n",
      "but now i’m left here feeling stupid\n",
      "oh the way he makes me feel\n",
      "that love isn’t real\n",
      "cupid is so dumb\n",
      "\n",
      "i look for his arrows everyday\n",
      "i guess he got lost or flew away\n",
      "waiting around is a waste (waste)\n",
      "been counting the days since november\n",
      "is loving as good as they say? now i’m so lonely (lonely)\n",
      "oh i wish i’d find a lover\n",
      "that could hold me (hold me)\n",
      "now i’m crying in my room\n",
      "so skeptical of love\n",
      "(say what you say but i want it more)\n",
      "but still i want it more, more, more\n",
      "\n",
      "i gave a second chance to cupid\n",
      "but now i’m left here feeling stupid\n",
      "oh the way he makes me feel\n",
      "that love isn’t real\n",
      "cupid is so dumb\n",
      "\n",
      "hopeless girl is seeking\n",
      "someone who will share this feeling\n",
      "i’m a fool, a fool for love, a fool for love\n",
      "\n",
      "i gave a second chance to cupid\n",
      "but now i’m left here feeling stupid\n",
      "oh the way he makes me feel\n",
      "that love isn’t real\n",
      "cupid is so dumb\n",
      "\n",
      "i gave a second chance to cupid\n",
      "but now i’m left here feeling stupid\n",
      "oh the way he makes me feel\n",
      "that love isn’t real\n",
      "cupid is so dumb \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YONSAI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\YONSAI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 문장 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 단어 토큰화\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    # 불용어 제거\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words, sentences\n",
    "\n",
    "def calculate_sentence_scores(words, sentences):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # 단어의 빈도수 계산\n",
    "    word_freq = FreqDist(words)\n",
    "    # 문장의 점수 계산\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = tokenizer.tokenize(sentence)\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words])\n",
    "        scores.append(sentence_score)\n",
    "    return scores\n",
    "\n",
    "def generate_summary(text):\n",
    "    words, sentences = preprocess_text(text)\n",
    "    scores = calculate_sentence_scores(words, sentences)\n",
    "    # 문장 점수 정규화\n",
    "    max_score = np.max(scores)\n",
    "    normalized_scores = [score / max_score for score in scores]\n",
    "    # 문장 추출\n",
    "    summary = \"\"\n",
    "    for sentence, score in zip(sentences, normalized_scores):\n",
    "        if score > 0.5:\n",
    "            summary += sentence + \" \"\n",
    "    return summary\n",
    "\n",
    "# 가사 입력 받기\n",
    "input_lyrics = '''A hopeless romantic all my life\n",
    "Surrounded by couples all the time\n",
    "I guess I should take it as a sign\n",
    "(oh why oh why, oh why oh why)\n",
    "\n",
    "I’m feeling lonely (lonely)\n",
    "Oh I wish I’d find a lover\n",
    "that could hold me (hold me)\n",
    "Now I’m crying in my room\n",
    "So skeptical of love\n",
    "(say what you say but I want it more)\n",
    "But still I want it more, more, more\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb\n",
    "\n",
    "I look for his arrows everyday\n",
    "I guess he got lost or flew away\n",
    "Waiting around is a waste (waste)\n",
    "Been counting the days since November\n",
    "Is loving as good as they say?\n",
    "\n",
    "Now I’m so lonely (lonely)\n",
    "Oh I wish I’d find a lover\n",
    "that could hold me (hold me)\n",
    "Now I’m crying in my room\n",
    "So skeptical of love\n",
    "(say what you say but I want it more)\n",
    "But still I want it more, more, more\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb\n",
    "\n",
    "Hopeless girl is seeking\n",
    "someone who will share this feeling\n",
    "I’m a fool, a fool for love, a fool for love\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb\n",
    "\n",
    "I gave a second chance to Cupid\n",
    "But now I’m left here feeling stupid\n",
    "Oh the way he makes me feel\n",
    "that love isn’t real\n",
    "Cupid is so dumb'''\n",
    "\n",
    "# 가사 요약 출력\n",
    "summary = generate_summary(input_lyrics)\n",
    "print(\"가사 요약:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래부터 bad, I'm so bad honestly, so bad Hey 전부 가질래 지금부턴 call you mine (mine) 도망칠 거면 don't cross my borderline (hey) (Line, line, line) 늘 뻔하기만 했던 everyday (day) 이제 그만 rule을 벗어날 때 어렵지 않아 just right now 'Cause I'm too spicy for your heart Ring the fire alarm 심장을 파고들어 넌 I'm too spicy 번지는 자극 속에 넌 바로 그 순간 또 다른 나를 발견해 I'm too spicy, too, too, I'm too spicy Ready?\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "def summarize_text(text, num_sentences=1):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summarizer.stop_words = get_stop_words(\"english\")\n",
    "    summary = summarizer(parser.document, num_sentences)\n",
    "    \n",
    "    result = []\n",
    "    for sentence in summary:\n",
    "        result.append(str(sentence))\n",
    "    \n",
    "    return \" \".join(result)\n",
    "\n",
    "# 사용 예시\n",
    "input_text = '''Uh, uh\n",
    "Too spicy\n",
    "You want my A to the Z\n",
    "But you won't 어림없지\n",
    "맞혀봐 sweet 또는 freak\n",
    "What's hiding underneath? (I see)\n",
    "망설이듯 간 보는 너 기회는 없어 oh\n",
    "No, you won't get it\n",
    "Nah, nah, you won't get it, I say (hey)\n",
    "깜빡 한순간 끌어당겨 you'll be mine (mine)\n",
    "살짝 다가와 can cross my borderline (hey)\n",
    "(Line, line, line) 널 따분하게 했던 everyday (day)\n",
    "흥미로운 덫을 던져줄게\n",
    "뛰어들어 봐 just right now\n",
    "'Cause I'm too spicy for your heart\n",
    "Ring the fire alarm\n",
    "심장을 파고들어 넌 I'm too spicy\n",
    "번지는 자극 속에 넌\n",
    "바로 그 순간\n",
    "또 다른 나를 발견해\n",
    "I'm too spicy, too, too, I'm too spicy\n",
    "Don't stop 겁내지 마 (hey)\n",
    "Bang, bang 외쳐봐\n",
    "I'm too spicy, yeah, I'm too spicy, you know that I\n",
    "Don't stop 용기 내 봐 (hey)\n",
    "Next step, myself\n",
    "I'm too spicy, too spicy, too, too, I'm too spicy\n",
    "Tell me what you see (see)\n",
    "When you look at me (me)\n",
    "'Cause I am a ten out of ten honestly\n",
    "기세가 다른 move (move) 널 압도하는 groove (groove)\n",
    "But you keep wasting your time, dudе\n",
    "Don't chase me 경고해 난\n",
    "Erase mе 멀리 달아나\n",
    "Hey 이젠 알겠니?\n",
    "원래부터 bad, I'm so bad honestly, so bad\n",
    "Hey 전부 가질래 지금부턴 call you mine (mine)\n",
    "도망칠 거면 don't cross my borderline (hey)\n",
    "(Line, line, line) 늘 뻔하기만 했던 everyday (day)\n",
    "이제 그만 rule을 벗어날 때\n",
    "어렵지 않아 just right now\n",
    "'Cause I'm too spicy for your heart\n",
    "Ring the fire alarm\n",
    "심장을 파고들어 넌 I'm too spicy\n",
    "번지는 자극 속에 넌\n",
    "바로 그 순간\n",
    "또 다른 나를 발견해\n",
    "I'm too spicy, too, too, I'm too spicy\n",
    "Ready? uh\n",
    "좀 더 강도를 높여 다음, 다음, 다음 (hoo)\n",
    "Ayy, ayy, ayy, one of a kind\n",
    "Ooh-hoo 우린 한계를 앞서 wow, wow, wow\n",
    "That's right, oh, oh, oh, oh\n",
    "'Cause I'm too spicy for your heart\n",
    "(Oh) ring the fire alarm\n",
    "새로운 도전 끝에 넌 (ooh) I'm too spicy\n",
    "변화할 시간이야 넌\n",
    "지금 이 순간 (so hot) 또 다른 나를 찾아내\n",
    "I'm too spicy, too, too, I'm too spicy\n",
    "Don't stop 겁내지 마 (hey)\n",
    "Bang, bang 외쳐봐\n",
    "I'm too spicy, yeah, I'm too spicy, you know that I\n",
    "Don't stop 용기 내 봐 (hey)\n",
    "Next step, myself\n",
    "I'm too spicy, too spicy, too, too, I'm too spicy\n",
    "'Cause I, oh, yeah\n",
    "Yeah, I'm too spicy for your heart\n",
    "(Yeah, I'm too spicy for your heart)\n",
    "마침내 번져오는 joy (enjoy, joy, joy, joy)\n",
    "Yeah, I'm too spicy for your heart\n",
    "I'm too spicy, too, too, I'm too spicy for you'''\n",
    "\n",
    "summary = summarize_text(input_text)\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
